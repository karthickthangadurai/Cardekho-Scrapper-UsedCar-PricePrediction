{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511 Second Hand Cars in Kolkata\n",
      "1511\n",
      "100 links completed\n",
      "100 links completed\n",
      "200 links completed\n",
      "200 links completed\n",
      "300 links completed\n",
      "300 links completed\n",
      "400 links completed\n",
      "500 links completed\n",
      "600 links completed\n",
      "700 links completed\n",
      "800 links completed\n",
      "900 links completed\n",
      "900 links completed\n",
      "1000 links completed\n",
      "1000 links completed\n",
      "1100 links completed\n",
      "1100 links completed\n",
      "1200 links completed\n",
      "1300 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n",
      "1400 links completed\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.149)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7787E7D12+55474]\n\t(No symbol) [0x00007FF7787577C2]\n\t(No symbol) [0x00007FF77860E0EB]\n\t(No symbol) [0x00007FF7785EE528]\n\t(No symbol) [0x00007FF778673B77]\n\t(No symbol) [0x00007FF7786875BF]\n\t(No symbol) [0x00007FF77866EF33]\n\t(No symbol) [0x00007FF778643D41]\n\t(No symbol) [0x00007FF778644F84]\n\tGetHandleVerifier [0x00007FF778B4B762+3609346]\n\tGetHandleVerifier [0x00007FF778BA1A80+3962400]\n\tGetHandleVerifier [0x00007FF778B99F0F+3930799]\n\tGetHandleVerifier [0x00007FF778883CA6+694342]\n\t(No symbol) [0x00007FF778762218]\n\t(No symbol) [0x00007FF77875E484]\n\t(No symbol) [0x00007FF77875E5B2]\n\t(No symbol) [0x00007FF77874EE13]\n\tBaseThreadInitThunk [0x00007FF925A3257D+29]\n\tRtlUserThreadStart [0x00007FF92730AA68+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32md:\\VS_Code\\Pro_1\\Final_CarDekho.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_Code/Pro_1/Final_CarDekho.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_Code/Pro_1/Final_CarDekho.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m#==============================================getting soup and count the links================================#\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VS_Code/Pro_1/Final_CarDekho.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(driver\u001b[39m.\u001b[39;49mpage_source, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_Code/Pro_1/Final_CarDekho.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m div_links_elements \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbottom_container\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VS_Code/Pro_1/Final_CarDekho.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(div_links_elements) \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Digital Suppliers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:445\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpage_source\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \n\u001b[0;32m    440\u001b[0m \u001b[39m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39m            driver.page_source\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET_PAGE_SOURCE)[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Digital Suppliers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:344\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 344\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    345\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Digital Suppliers\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=117.0.5938.149)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7787E7D12+55474]\n\t(No symbol) [0x00007FF7787577C2]\n\t(No symbol) [0x00007FF77860E0EB]\n\t(No symbol) [0x00007FF7785EE528]\n\t(No symbol) [0x00007FF778673B77]\n\t(No symbol) [0x00007FF7786875BF]\n\t(No symbol) [0x00007FF77866EF33]\n\t(No symbol) [0x00007FF778643D41]\n\t(No symbol) [0x00007FF778644F84]\n\tGetHandleVerifier [0x00007FF778B4B762+3609346]\n\tGetHandleVerifier [0x00007FF778BA1A80+3962400]\n\tGetHandleVerifier [0x00007FF778B99F0F+3930799]\n\tGetHandleVerifier [0x00007FF778883CA6+694342]\n\t(No symbol) [0x00007FF778762218]\n\t(No symbol) [0x00007FF77875E484]\n\t(No symbol) [0x00007FF77875E5B2]\n\t(No symbol) [0x00007FF77874EE13]\n\tBaseThreadInitThunk [0x00007FF925A3257D+29]\n\tRtlUserThreadStart [0x00007FF92730AA68+40]\n"
     ]
    }
   ],
   "source": [
    "#=====================================cardekho.com website scrapping the links=================================#\n",
    "#import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Creating a webdriver instance\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.cardekho.com/used-cars+in+kolkata\")\n",
    "driver.maximize_window()\n",
    "driver.implicitly_wait(10)\n",
    " \n",
    "#=====================================clicking the city name================================================#        \n",
    "driver.find_element(By.XPATH,\"//*[@alt='Kolkata']\").click()\n",
    "\n",
    "#====================================finding the total cars=================================================#\n",
    "total_cars_css = 'div.headingSection>h1'\n",
    "total_cars_text = driver.find_element(By.CSS_SELECTOR,total_cars_css).text\n",
    "print(total_cars_text)\n",
    "print(int(\"\".join(re.findall(r'\\d+', total_cars_text))))\n",
    "\n",
    "#path = \"div.bottom_container>div>div>h3>a\"\n",
    "#container_path = \"div.bottom_container\"\n",
    "\n",
    "#=================Get scrolled till get to the bottom and get 1500 links in each city============================#\n",
    "\n",
    "element =driver.find_element(By.TAG_NAME,\"body\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    element.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #==============================================getting soup and count the links================================#\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    div_links_elements = soup.find_all('div', class_=\"bottom_container\")\n",
    "    \n",
    "    if len(div_links_elements) % 100 ==0:\n",
    "        print(f\"{len(div_links_elements)} links completed\")\n",
    "    #=======================================if the total links in the city is more than 1500 then break============#\n",
    "    if len(div_links_elements) >1400:\n",
    "        break\n",
    "    #=======================================if find the element then break the loops================================#\n",
    "    # stop_path = \"div.toggleAccordion.UCCaccordion\"\n",
    "    \n",
    "    # if driver.find_element(By.CSS_SELECTOR, stop_path).is_displayed():\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 links colected\n"
     ]
    }
   ],
   "source": [
    "#===============================getting the links from the soup and making csv====================================#\n",
    "div_elements = soup.find_all('div', class_=\"bottom_container\")\n",
    "\n",
    "print(f\"{len(div_elements)} links colected\") \n",
    "#==============================storing the links in a dictionary=================================================#\n",
    "car_link_dict = { \"car_titles\" : [i.find('h3').text for i in div_elements], \n",
    "                 \"car_links\" : [ \"https://www.cardekho.com\"+i.find('a').get('href') for i in div_elements]}\n",
    "\n",
    "#==============================storing the links in a csv file===================================================#\n",
    "link_frame = pd.DataFrame.from_dict(car_link_dict)\n",
    "\n",
    "link_frame.to_csv(r\"D:\\scrapped_jobs\\cardekho_data\\CarLinks\\kolkata_car_links.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cardekho.com/used-car-details/used-Toyota-Camry-Hybrid-cars-Kolkata_0ece1255-6096-4708-8666-3d5dab9971b4.htm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#==============================read the links from csv file======================================================#\n",
    "links = pd.read_csv(r\"D:\\scrapped_jobs\\cardekho_data\\CarLinks\\kolkata_car_links.csv\")['car_links'].to_list()\n",
    "print(links[0])\n",
    "\n",
    "#==================================adding scrapped and not scrapped links=======================================#\n",
    "add_links = []\n",
    "not_scrapped_links = {'error_links':[]}\n",
    "\n",
    "#=========================storig the car details in a dictionary================================================#\n",
    "new_car_details = {\"new_car_detail\":[],\"new_car_overview\" :[],\"new_car_feature\" :[],\"new_car_specs\":[]}\n",
    "required_keys = ['carDetails','carOverview','carFeatures','carSpecification']\n",
    "\n",
    "wait_time  = 1\n",
    "no_scrap_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 links scraped and completed. 1 links not scraped. Standing at position 48\n",
      "100 links scraped and completed. 1 links not scraped. Standing at position 98\n",
      "150 links scraped and completed. 1 links not scraped. Standing at position 148\n",
      "200 links scraped and completed. 1 links not scraped. Standing at position 198\n",
      "250 links scraped and completed. 1 links not scraped. Standing at position 248\n",
      "300 links scraped and completed. 1 links not scraped. Standing at position 298\n",
      "350 links scraped and completed. 1 links not scraped. Standing at position 359\n",
      "400 links scraped and completed. 1 links not scraped. Standing at position 410\n",
      "450 links scraped and completed. 1 links not scraped. Standing at position 460\n",
      "500 links scraped and completed. 1 links not scraped. Standing at position 510\n",
      "550 links scraped and completed. 1 links not scraped. Standing at position 562\n",
      "600 links scraped and completed. 1 links not scraped. Standing at position 613\n",
      "650 links scraped and completed. 1 links not scraped. Standing at position 663\n",
      "700 links scraped and completed. 1 links not scraped. Standing at position 715\n",
      "750 links scraped and completed. 1 links not scraped. Standing at position 765\n",
      "800 links scraped and completed. 1 links not scraped. Standing at position 816\n",
      "850 links scraped and completed. 1 links not scraped. Standing at position 867\n",
      "900 links scraped and completed. 1 links not scraped. Standing at position 917\n",
      "950 links scraped and completed. 1 links not scraped. Standing at position 967\n",
      "1000 links scraped and completed. 1 links not scraped. Standing at position 1017\n",
      "1050 links scraped and completed. 1 links not scraped. Standing at position 1067\n",
      "1100 links scraped and completed. 1 links not scraped. Standing at position 1117\n",
      "1150 links scraped and completed. 1 links not scraped. Standing at position 1167\n",
      "1200 links scraped and completed. 1 links not scraped. Standing at position 1217\n",
      "1250 links scraped and completed. 1 links not scraped. Standing at position 1267\n",
      "1300 links scraped and completed. 1 links not scraped. Standing at position 1317\n",
      "1350 links scraped and completed. 1 links not scraped. Standing at position 1367\n"
     ]
    }
   ],
   "source": [
    "#==================================getting the car details===========================================================#\n",
    "#==================================import required libraries=========================================================#\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#==================================getting the links from the link list and iterate===================================#\n",
    "\n",
    "for index,url in enumerate(links):\n",
    "    #=============================checking the url is already scrapped or not========================================#\n",
    "    if url not in add_links:\n",
    "        \n",
    "        try:\n",
    "    # Creating a webdriver instance\n",
    "            #driver = webdriver.Chrome()\n",
    "            op = webdriver.ChromeOptions() #giving headless options to not show the browser\n",
    "            op.add_argument('headless')\n",
    "            driver = webdriver.Chrome(options=op)\n",
    "            driver.get(url)\n",
    "            driver.maximize_window()\n",
    "            driver.minimize_window()  \n",
    "            driver.maximize_window()  \n",
    "            driver.switch_to.window(driver.current_window_handle)\n",
    "            driver.implicitly_wait(10)\n",
    "            \n",
    "            #=============================clicking the city name=============================================#\n",
    "            driver.find_element(By.XPATH,\"//*[@alt='Kolkata']\").click()\n",
    "            #=============================getting the soup===================================================#\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            #==============getting the car details which is present in window.__INITIAL_STATE__=================#\n",
    "            #adding paranthesis to the string to get the data in json format\n",
    "            data = \"{\" + re.search(r\"window.__INITIAL_STATE__ = {(.*?)};\", soup.prettify()).group(1) + '}'\n",
    "            data = json.loads(data)\n",
    "            \n",
    "            #============================getting the car details and append it==================================#\n",
    "            #cardetails, overview, features, specs, links and close() the driver\n",
    "            for title_keys,keys in zip(new_car_details,required_keys):\n",
    "                new_car_details[title_keys].append(data['item'][keys])\n",
    "            \n",
    "            add_links.append(url)    \n",
    "            driver.close()\n",
    "            #time.sleep(1)\n",
    "            \n",
    "            wait_time += 1\n",
    "            \n",
    "            if wait_time%50 == 0:\n",
    "                time.sleep(5)\n",
    "                print(f\"\"\"{wait_time} links scraped and completed. {no_scrap_count} links not scraped. Standing at position {index}\"\"\")\n",
    "        #=============================catching the error if any and close the driver========================================#\n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "            not_scrapped_links['error_links'].append(url)\n",
    "            driver.close()\n",
    "        \n",
    "            if no_scrap_count%50 == 0:\n",
    "                print(f\"{no_scrap_count} links not scraped\")\n",
    "            \n",
    "            no_scrap_count += 1\n",
    "            time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381\n",
      "{'error_links': []}\n"
     ]
    }
   ],
   "source": [
    "print(len(add_links))\n",
    "print(not_scrapped_links)\n",
    "\n",
    "#==============================update the links in dictionary and convert into csv file====================================#\n",
    "new_car_details.update({\"car_links\":add_links})\n",
    "\n",
    "cardf=pd.DataFrame(new_car_details)\n",
    "cardf.to_csv(r\"D:\\scrapped_jobs\\cardekho_data\\Scrapped_Cars_Data\\kolkata_cars.csv\",encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698\n"
     ]
    }
   ],
   "source": [
    "print(len(add_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
